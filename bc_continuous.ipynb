{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e73ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fc754f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gymnasium import spaces\n",
    "from rl_zoo3.train import train\n",
    "from stable_baselines3 import PPO\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import minari\n",
    "from minari import DataCollector\n",
    "\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "213bc0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.argv = [\"python\", \"--algo\", \"ppo\", \"--env\", \"CartPole-v1\"]\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b068e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = DataCollector(gym.make('CartPole-v1'))\n",
    "# path = os.path.abspath('') + '/logs/ppo/CartPole-v1_1/best_model'\n",
    "# agent = PPO.load(path)\n",
    "\n",
    "# total_episodes = 1_000\n",
    "# # total_episodes = 1\n",
    "\n",
    "# for i in tqdm(range(total_episodes)):\n",
    "#     obs, _ = env.reset(seed=42)\n",
    "#     while True:\n",
    "#         action, _ = agent.predict(obs)\n",
    "#         obs, rew, terminated, truncated, info = env.step(action)\n",
    "\n",
    "#         if terminated or truncated:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "705197e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = env.create_dataset(\n",
    "#     dataset_id=\"cartpole/expert-v0\",\n",
    "#     algorithm_name=\"ExpertPolicy\",\n",
    "#     code_permalink=\"https://minari.farama.org/tutorials/behavioral_cloning\",\n",
    "#     author=\"Farama\",\n",
    "#     author_email=\"contact@farama.org\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbcb24f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d68a3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"id\": torch.Tensor([x.id for x in batch]),\n",
    "        \"observations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.observations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"actions\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.actions) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"rewards\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.rewards) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"terminations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.terminations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"truncations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.truncations) for x in batch],\n",
    "            batch_first=True\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7875290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/64, Loss: 1542.4703369140625\n",
      "Epoch: 1/64, Loss: 1530.2423095703125\n",
      "Epoch: 2/64, Loss: 1517.13037109375\n",
      "Epoch: 3/64, Loss: 1503.80517578125\n",
      "Epoch: 4/64, Loss: 1491.7354736328125\n",
      "Eval after epoch 5: reward = 29.0\n",
      "Epoch: 5/64, Loss: 1482.278076171875\n",
      "Epoch: 6/64, Loss: 1475.798095703125\n",
      "Epoch: 7/64, Loss: 1472.278564453125\n",
      "Epoch: 8/64, Loss: 1469.7216796875\n",
      "Epoch: 9/64, Loss: 1468.1065673828125\n",
      "Eval after epoch 10: reward = 56.0\n",
      "Epoch: 10/64, Loss: 1467.03759765625\n",
      "Epoch: 11/64, Loss: 1465.8514404296875\n",
      "Epoch: 12/64, Loss: 1464.6651611328125\n",
      "Epoch: 13/64, Loss: 1463.3984375\n",
      "Epoch: 14/64, Loss: 1462.1812744140625\n",
      "Eval after epoch 15: reward = 98.0\n",
      "Epoch: 15/64, Loss: 1461.126708984375\n",
      "Epoch: 16/64, Loss: 1460.30712890625\n",
      "Epoch: 17/64, Loss: 1459.1807861328125\n",
      "Epoch: 18/64, Loss: 1458.323486328125\n",
      "Epoch: 19/64, Loss: 1457.716552734375\n",
      "Eval after epoch 20: reward = 123.0\n",
      "Epoch: 20/64, Loss: 1456.4222412109375\n",
      "Epoch: 21/64, Loss: 1455.5244140625\n",
      "Epoch: 22/64, Loss: 1455.338623046875\n",
      "Epoch: 23/64, Loss: 1454.5909423828125\n",
      "Epoch: 24/64, Loss: 1453.7796630859375\n",
      "Eval after epoch 25: reward = 142.0\n",
      "Epoch: 25/64, Loss: 1453.0758056640625\n",
      "Epoch: 26/64, Loss: 1453.177490234375\n",
      "Epoch: 27/64, Loss: 1452.6094970703125\n",
      "Epoch: 28/64, Loss: 1452.023681640625\n",
      "Epoch: 29/64, Loss: 1451.6915283203125\n",
      "Eval after epoch 30: reward = 153.0\n",
      "Epoch: 30/64, Loss: 1451.484130859375\n",
      "Epoch: 31/64, Loss: 1450.909423828125\n",
      "Epoch: 32/64, Loss: 1450.883056640625\n",
      "Epoch: 33/64, Loss: 1450.37939453125\n",
      "Epoch: 34/64, Loss: 1450.28564453125\n",
      "Eval after epoch 35: reward = 193.0\n",
      "Epoch: 35/64, Loss: 1449.044189453125\n",
      "Epoch: 36/64, Loss: 1449.0723876953125\n",
      "Epoch: 37/64, Loss: 1448.398193359375\n",
      "Epoch: 38/64, Loss: 1448.0782470703125\n",
      "Epoch: 39/64, Loss: 1447.66845703125\n",
      "Eval after epoch 40: reward = 321.0\n",
      "Epoch: 40/64, Loss: 1447.1900634765625\n",
      "Epoch: 41/64, Loss: 1446.734619140625\n",
      "Epoch: 42/64, Loss: 1446.4080810546875\n",
      "Epoch: 43/64, Loss: 1445.9583740234375\n",
      "Epoch: 44/64, Loss: 1445.487548828125\n",
      "Eval after epoch 45: reward = 500.0\n",
      "Epoch: 45/64, Loss: 1444.9859619140625\n",
      "Epoch: 46/64, Loss: 1444.6927490234375\n",
      "Epoch: 47/64, Loss: 1444.01171875\n",
      "Epoch: 48/64, Loss: 1443.2486572265625\n",
      "Epoch: 49/64, Loss: 1443.3203125\n",
      "Eval after epoch 50: reward = 500.0\n",
      "Epoch: 50/64, Loss: 1442.470947265625\n",
      "Epoch: 51/64, Loss: 1441.7960205078125\n",
      "Epoch: 52/64, Loss: 1441.6077880859375\n",
      "Epoch: 53/64, Loss: 1441.07958984375\n",
      "Epoch: 54/64, Loss: 1440.629638671875\n",
      "Eval after epoch 55: reward = 500.0\n",
      "Epoch: 55/64, Loss: 1440.656005859375\n",
      "Epoch: 56/64, Loss: 1439.91552734375\n",
      "Epoch: 57/64, Loss: 1439.732177734375\n",
      "Epoch: 58/64, Loss: 1439.74462890625\n",
      "Epoch: 59/64, Loss: 1439.4229736328125\n",
      "Eval after epoch 60: reward = 500.0\n",
      "Epoch: 60/64, Loss: 1438.850830078125\n",
      "Epoch: 61/64, Loss: 1438.39892578125\n",
      "Epoch: 62/64, Loss: 1437.7825927734375\n",
      "Epoch: 63/64, Loss: 1437.8948974609375\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "NUM_EPOCHS = 64\n",
    "EVAL_EVERY_N_EPOCHS = 5\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "minari_dataset = minari.load_dataset(\"cartpole/expert-v0\")\n",
    "dataloader = DataLoader(minari_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "env = minari_dataset.recover_environment()\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "assert isinstance(observation_space, spaces.Box)\n",
    "assert isinstance(action_space, spaces.Discrete)\n",
    "\n",
    "policy_net = PolicyNetwork(np.prod(observation_space.shape), action_space.n)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set up eval environment for periodic testing\n",
    "eval_env = minari_dataset.recover_environment()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in dataloader:\n",
    "        a_pred = policy_net(batch['observations'][:, :-1])\n",
    "        a_hat = F.one_hot(batch[\"actions\"].type(torch.int64))\n",
    "        loss = loss_fn(a_pred, a_hat.type(torch.float32))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}/{NUM_EPOCHS}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Periodic evaluation\n",
    "    if (epoch + 1) % EVAL_EVERY_N_EPOCHS == 0:\n",
    "        obs, _ = eval_env.reset(seed=42)\n",
    "        terminated, truncated = False, False\n",
    "        accumulated_rew = 0\n",
    "        while not (terminated or truncated):\n",
    "            action = policy_net(torch.Tensor(obs)).argmax().item()\n",
    "            obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "            accumulated_rew += reward\n",
    "        print(f\"Eval after epoch {epoch+1}: reward = {accumulated_rew}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f38ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated rew:  500.0\n",
      "Video saved to: videos/offline/bc/CartPole-v1/2025-11-30_125131/epoch=64_reward=500.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "\n",
    "# Set up directories and video saving vars\n",
    "env_name = \"CartPole-v1\"\n",
    "date_str = now.strftime(\"%Y-%m-%d\")\n",
    "time_str = now.strftime(\"%H%M%S\")\n",
    "base_dir = f\"videos/offline/bc/{env_name}/{date_str}_{time_str}\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "video_path = os.path.join(\n",
    "    base_dir,\n",
    "    f\"epoch={epoch+1}_reward={{REWARD_PLACEHOLDER}}.mp4\"\n",
    ")\n",
    "# Note: reward is not known yet at time of instantiation, so we'll rename at the end\n",
    "\n",
    "env = gym.make(\n",
    "    \"CartPole-v1\",\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "frames = []\n",
    "obs, _ = env.reset(seed=42)\n",
    "terminated, truncated = False, False\n",
    "accumulated_rew = 0\n",
    "while not (terminated or truncated):\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "    action = policy_net(torch.Tensor(obs)).argmax()\n",
    "    obs, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "    accumulated_rew += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "final_path = os.path.join(\n",
    "    base_dir,\n",
    "    f\"epoch={NUM_EPOCHS}_reward={int(accumulated_rew)}.mp4\"\n",
    ")\n",
    "imageio.mimsave(final_path, frames, fps=30)\n",
    "\n",
    "print(\"Accumulated rew: \", accumulated_rew)\n",
    "print(\"Video saved to:\\n\", final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b0c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
